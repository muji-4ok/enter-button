{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data from: https://github.com/ravsimar-sodhi/jibes-and-delights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import signal\n",
    "interrupt = [False]\n",
    "\n",
    "def sig_handler(signum, frame):\n",
    "    interrupt[0] = True\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68159\n",
      "During inclement weather does your hair function as a umbrella?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = ['comment.dev.0', 'comment.test.0', 'comment.train.0']\n",
    "\n",
    "text = []\n",
    "for f in files:\n",
    "    with open(f, 'r') as iofile:\n",
    "        text += iofile.readlines()\n",
    "\n",
    "print(len(text))\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68159\n",
      "during inclement weather does your hair function as a umbrella?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [line.lower() for line in text]\n",
    "print(len(text))\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(min([len(line) for line in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '¬£', '¬©', '¬Æ', '¬∞', '¬¥', '√¢', '√§', '√¶', '√®', '√©', '√™', '√´', '√¨', '√≠', '√±', '√∂', '√º', ' ñ', 'Õú', 'Õ†', 'Õ°', 'Œµ', 'œÄ', '–±', '–µ', '–∂', '–π', '–º', '–æ', 'ŸÑ', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¶', '‚Ñ¢', '‚òÖ', '‚òë', '‚ô•', '‚ô™', '‚úÑ', '‚úÖ', '‚úì', '‚úî', '‚¨á', 'Ô∏è', 'Ôøº', 'üåö', 'üçÑ', 'üé≥', 'üèá', 'üëè', 'üíï', 'üí©', 'üòÇ', 'üòÉ', 'üòà', 'üòú']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(set(''.join(text)))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self, tokens, embedding_size=16, hidden_dim_size=64, num_layers=1):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.token_to_id = dict([(y, x) for (x, y) in enumerate(self.tokens, 0)])\n",
    "        self.id_to_token = dict(enumerate(self.tokens, 0))\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(self.tokens), embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_dim_size, num_layers=num_layers, batch_first=True)\n",
    "        self.rnn_to_logits = nn.Linear(hidden_dim_size, len(self.tokens))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "        h_seq, _ = self.rnn(self.embedding(x))\n",
    "        next_logits = self.rnn_to_logits(h_seq)\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentNeuralNetwork(tokens, 50, 250, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader([[model.token_to_id[char] for char in line] for line in text], batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_80047/3794864350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterrupt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "interrupt = [False]\n",
    "signal.signal(signal.SIGINT, sig_handler)\n",
    "while not interrupt[0]:\n",
    "    for data in dataloader:\n",
    "        opt.zero_grad()\n",
    "        batch_ix = torch.stack(data).transpose(0, 1)\n",
    "\n",
    "        logp_seq = model(batch_ix)\n",
    "\n",
    "        predictions_logp = logp_seq[:, :-1].reshape(-1, len(tokens))\n",
    "        actual_next_tokens = batch_ix[:, 1:].reshape(-1)\n",
    "\n",
    "        loss_res = loss(predictions_logp, actual_next_tokens)\n",
    "\n",
    "        # train with backprop\n",
    "        loss_res.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    history_model.append(loss_res.data)\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history_model,label='loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5517989\n"
     ]
    }
   ],
   "source": [
    "new_text = ''.join(text)\n",
    "print(len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork():\n",
    "    def __init__(self, text, embedding_size=16, hidden_dim_size=64, num_layers=1):\n",
    "        class LSTM_RNN(nn.Module):\n",
    "            def __init__(self, vocab_size, embedding_size, hidden_dim_size):\n",
    "                super(self.__class__,self).__init__()\n",
    "                self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "                self.rnn = nn.LSTM(embedding_size, hidden_dim_size, num_layers=num_layers, batch_first=True)\n",
    "                self.rnn_to_logits = nn.Linear(hidden_dim_size, vocab_size)\n",
    "\n",
    "            def forward(self, x):\n",
    "                assert isinstance(x.data, torch.LongTensor)\n",
    "                h_seq, _ = self.rnn(self.embedding(x))\n",
    "                next_logits = self.rnn_to_logits(h_seq)\n",
    "                next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "                return next_logp\n",
    "        self.text = ''.join(text).lower()\n",
    "        self.tokens = sorted(set(self.text))\n",
    "        self.token_to_id = dict([(y, x) for (x, y) in enumerate(self.tokens, 0)])\n",
    "        self.id_to_token = dict(enumerate(self.tokens, 0))\n",
    "        self.model = LSTM_RNN(len(self.tokens), embedding_size, hidden_dim_size)\n",
    "        self.opt = torch.optim.Adam(self.model.parameters())\n",
    "        self.loss = nn.NLLLoss()\n",
    "    \n",
    "    def train_cycle(self, data_loader, history):\n",
    "        for data in data_loader:\n",
    "            self.opt.zero_grad()\n",
    "            batch_ix = torch.stack(data).transpose(0, 1)\n",
    "\n",
    "            logp_seq = self.model(batch_ix)\n",
    "\n",
    "            predictions_logp = logp_seq[:, :-1].reshape(-1, len(self.tokens))\n",
    "            actual_next_tokens = batch_ix[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = self.loss(predictions_logp, actual_next_tokens)\n",
    "\n",
    "            # train with backprop\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "        history.append(loss.data)\n",
    "        \n",
    "    def generate_sample(self, seed_phrase, length=100, temperature=1.0):\n",
    "        x_sequence = [self.token_to_id[char] for char in seed_phrase]\n",
    "        x_sequence = torch.tensor([x_sequence])\n",
    "\n",
    "        while x_sequence[-1] != '\\n' and len(x_sequence) < lenght:\n",
    "            out = self.model(x_sequence)[:, -1, :]\n",
    "            p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
    "\n",
    "            next_ix = np.random.choice(len(self.tokens), p=p_next)\n",
    "            next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "            x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "\n",
    "        return ''.join([self.id_to_token[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, model, seq_length, step=1):\n",
    "        self.text = [model.token_to_id[char] for char in model.text]\n",
    "        self.seq_length = seq_length\n",
    "        self.step =step\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = -self.step\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.idx += self.step\n",
    "        if self.idx <= len(self.text) - seq_length:\n",
    "            return self.text[self.idx : self.idx + seq_length]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "seq_length = 16\n",
    "step = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentNeuralNetwork(text=text, embedding_size=50, hidden_dim_size=250, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(TextDataset(model, seq_length, step), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "interrupt = [False]\n",
    "signal.signal(signal.SIGINT, sig_handler)\n",
    "while not interrupt[0]:\n",
    "    model.train_cycle(data_loader, history_model)\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(history_model,label='loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(model, seed_phrase, length=100, temperature=1.0):\n",
    "    x_sequence = [model.token_to_id[char] for char in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence])\n",
    "\n",
    "    while x_sequence[-1] != '\\n' and len(x_sequence) < length:\n",
    "        out = model.model(x_sequence)[:, -1, :]\n",
    "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
    "\n",
    "        next_ix = np.random.choice(len(model.tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        print(model.id_to_token[next_ix])\n",
    "\n",
    "    return ''.join([model.id_to_token[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([[43]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_80047/1049406642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_80047/4139746656.py\u001b[0m in \u001b[0;36mgenerate_sample\u001b[0;34m(model, seed_phrase, length, temperature)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnext_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([[43]])"
     ]
    }
   ],
   "source": [
    "generate_sample(model, 'you ', length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self, tokens, embedding_size=16, hidden_dim_size=64, num_layers=1):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.tokens = tokens\n",
    "        self.token_to_id = dict([(y, x) for (x, y) in enumerate(self.tokens, 0)])\n",
    "        self.id_to_token = dict(enumerate(self.tokens, 0))\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(self.tokens), embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_dim_size, num_layers=num_layers, batch_first=True)\n",
    "        self.rnn_to_logits = nn.Linear(hidden_dim_size, len(self.tokens))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "        h_seq, _ = self.rnn(self.embedding(x))\n",
    "        next_logits = self.rnn_to_logits(h_seq)\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, model, seq_length, step=1):\n",
    "        self.text = [model.token_to_id[char] for char in model.text]\n",
    "        self.seq_length = seq_length\n",
    "        self.step =step\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idx = -self.step\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.idx += self.step\n",
    "        if self.idx <= len(self.text) - seq_length:\n",
    "            return self.text[self.idx : self.idx + seq_length]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "\n",
    "seq_length = 16\n",
    "step = 16\n",
    "data_loader = torch.utils.data.DataLoader(TextDataset(new_model, seq_length, step), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentNeuralNetwork(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "def train_cycle(model, opt, loss, data_loader, history):\n",
    "    for data in data_loader:\n",
    "        opt.zero_grad()\n",
    "        batch_ix = torch.stack(data).transpose(0, 1)\n",
    "\n",
    "        logp_seq = model(batch_ix)\n",
    "\n",
    "        predictions_logp = logp_seq[:, :-1].reshape(-1, len(model.tokens))\n",
    "        actual_next_tokens = batch_ix[:, 1:].reshape(-1)\n",
    "\n",
    "        loss_res = loss(predictions_logp, actual_next_tokens)\n",
    "\n",
    "        # train with backprop\n",
    "        loss_res.backward()\n",
    "        opt.step()\n",
    "    history.append(loss_res.data)\n",
    "\n",
    "def generate_sample(model, seed_phrase, temperature=1.0):\n",
    "    x_sequence = [model.token_to_id[char] for char in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence])\n",
    "\n",
    "    while x_sequence[-1] != '\\n':\n",
    "        out = model(x_sequence)[:, -1, :]\n",
    "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
    "\n",
    "        next_ix = np.random.choice(len(model.tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "\n",
    "    return ''.join([model.id_to_token[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
